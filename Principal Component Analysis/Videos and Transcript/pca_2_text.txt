In the last video, we set up the PCA objective. And in this video, we will determine our first set of optimal parameters. We make two general assumptions in the beginning. The first thing is that we have centred data. That means that the expected value of our data set is zero. And the second one is that the basis vectors form an orthonormal basis. From the previous video, we carry over the following results. First, we can write our projected data point X_n tilde as a linear combination B_j_n times b_j, where b_j form the orthonormal basis of our subspace. Our loss function is the average squared reconstruction error between our original data point and the projection. And the partial derivative of our loss function with respect to X_n tilde is given by this expression. And now we are ready to compute the partial derivative of J with respect to the beta i_n parameters as follows. So d_J with respect to d_B_i_n is the derivative of J with respect to X_n tilde times the derivative of X_n tilde with respect to beta I_n. So now we are going to have a closer look at this one here. So d_X_n tilde by d beta i_n is simply given by b_i for i equals to one to M. And the reason for this is that, if we take the derivative with respect to one fixed beta i_n, then only the ith component of this sum will play a role, and that's a reason why we end up simply with b_i. But that also means that our derivative of J with respect to d_B_i_n is now given as d_j by d_B_i_n is minus two over n times X_n minus X_n tilde transpose times b_i where here we used equation C to get the first part, and we plugged in this bi_ over here. And what we're going to do now is we are going to replace X_n tilde using equation A. So we end up with minus two over n times X_n minus the sum of J equals one to n B_j_n times. So B_j_n times b_j transpose times b_i. And this is given as minus two over n times X_n transpose times b_i minus B_ i_n times b_i transpose times b_i where we exploited that the b_i form an orthogonal basis. If we multiply b_i onto both components here, we end up with the sum of b_j_n times b_j transpose times b_i. And since b_j transpose times b_i is one if and only if i equals j and otherwise zero, we end up with only one term which would be one. So we end up with minus two N times X_n transpose b_i minus B_ i_n. So now we need to set this to zero in order to find our B_i_n parameters and this is zero if and only if the B_ i_n parameters are given by X_n transpose times b_i which we are going to define as equation D. What this means is that the optimal coordinates of X_n tilde with respect to our basis are the orthogonal projections of the coordinates of our original data point onto the ith basis vector that spans our principal subspace. In this video, we determined the coordinates of a lower dimensional data as the orthogonal projection of the original data onto the basis vectors that span the principal subspace. In the next videos, we will determine the orthonormal basis that spans that principle subspace.